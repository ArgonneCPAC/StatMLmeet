{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate TF2 behavior:\n",
    "from tensorflow.python import tf2\n",
    "if not tf2.enabled():\n",
    "  import tensorflow.compat.v2 as tf\n",
    "  tf.enable_v2_behavior()\n",
    "  assert tf2.enabled()\n",
    "\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(10)\n",
    "# tf.random.set_seed(10)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Lambda, Dropout, Flatten, Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras import optimizers, models, regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model, Sequential, Model\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mode = 'train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100 #100 #5000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4 #1e-4\n",
    "decay_rate = 1e-2\n",
    "\n",
    "latent_dim = 512 #2*2*2\n",
    "epsilon_mean = 0.1\n",
    "epsilon_std = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_data = np.log10(np.load(\"../data/Zeldotest.np.npy\"))\n",
    "# output = (output - output.min() )/(output.max() - output.min())\n",
    "\n",
    "# swe_train = output[0:800,:,:]\n",
    "# swe_valid = output[800:1000,:,:]\n",
    "\n",
    "swe_data = swe_data.reshape(1000,64*64)\n",
    "# swe_data = (swe_data - np.min(swe_data))/(swe_data.max() - swe_data.min())\n",
    "# swe_valid = swe_valid.reshape(200,64,64,1)\n",
    "\n",
    "\n",
    "preproc = Pipeline([('stdscaler', StandardScaler())])\n",
    "\n",
    "swe_train = preproc.fit_transform(swe_data[:900,:])\n",
    "swe_valid = preproc.transform(swe_data[900:,:])\n",
    "# swe_train = swe_data[:900,:]\n",
    "# swe_valid = swe_data[900:,:]\n",
    "swe_train = swe_train.reshape(900,64,64,1)\n",
    "swe_valid = swe_valid.reshape(100,64,64,1)\n",
    "\n",
    "# Shuffle - to preserve the order of the initial dataset\n",
    "swe_train_data = np.copy(swe_train)\n",
    "swe_valid_data = np.copy(swe_valid)\n",
    "\n",
    "np.random.shuffle(swe_train_data)\n",
    "np.random.shuffle(swe_valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(swe_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(swe_valid[20, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_valid_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_def():\n",
    "    \n",
    "    def coeff_determination(y_pred, y_true): #Order of function inputs is important here        \n",
    "        SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "        SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "        return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "    # reparameterization trick\n",
    "    # instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "    # then z = z_mean + sqrt(var)*eps\n",
    "    def sampling(args):\n",
    "        \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "         Arguments\n",
    "            args (tensor): mean and log of variance of Q(z|X)\n",
    "         Returns\n",
    "            z (tensor): sampled latent vector\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        # by default, random_normal has mean=0 and std=1.0\n",
    "        epsilon = K.random_normal(shape=(batch, dim), mean=epsilon_mean, stddev=epsilon_std)\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    ## Encoder\n",
    "    encoder_inputs = Input(shape=(64,64,1),name='Field')\n",
    "    # Encode   \n",
    "#     x = Conv2D(256,kernel_size=(5,5),activation='relu',padding='same')(encoder_inputs)\n",
    "    x = Conv2D(256,kernel_size=(3,3),activation='relu',padding='same')(encoder_inputs)\n",
    "    x = Conv2D(128,kernel_size=(2,2),activation='relu',padding='same')(encoder_inputs)\n",
    "    x = Conv2D(128,kernel_size=(1,1),activation='relu',padding='same')(encoder_inputs)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "    x = Conv2D(128,kernel_size=(3,3),activation='relu',padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "#     x = Conv2D(64,kernel_size=(3,3),activation='relu',padding='same')(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "#     x = Conv2D(15,kernel_size=(3,3),activation='relu',padding='same')(enc_l4)\n",
    "#     enc_l5 = MaxPooling2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "#     x = Conv2D(10,kernel_size=(3,3),activation=None,padding='same')(enc_l5)\n",
    "#     encoded = MaxPooling2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "    # instantiate encoder model\n",
    "    encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    # build decoder model\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(512)(latent_inputs)\n",
    "    x = Reshape((16, 16, 2))(x)\n",
    "       \n",
    "#     x = Conv2D(2,kernel_size=(3,3),activation=None,padding='same')(x)\n",
    "#     dec_l1 = UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "#     x = Conv2D(15,kernel_size=(3,3),activation='relu',padding='same')(dec_l1)\n",
    "#     dec_l2 = UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "#     x = Conv2D(64,kernel_size=(3,3),activation='relu',padding='same')(x)\n",
    "#     x = UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(128,kernel_size=(3,3),activation='relu',padding='same')(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "#     x = Conv2D(256,kernel_size=(5,5),activation='relu',padding='same')(x)\n",
    "    x = Conv2D(256,kernel_size=(1,1),activation='relu',padding='same')(x)\n",
    "    x = Conv2D(256,kernel_size=(2,2),activation='relu',padding='same')(x)\n",
    "    x = Conv2D(256,kernel_size=(3,3),activation='relu',padding='same')(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "    decoded = Conv2D(1,kernel_size=(3,3),activation=None,padding='same')(x)\n",
    "    decoder = Model(inputs=latent_inputs,outputs=decoded)\n",
    "    decoder.summary()\n",
    "    # instantiate VAE model\n",
    "    ae_outputs = decoder(encoder(encoder_inputs))\n",
    "    model = Model(inputs=encoder_inputs,outputs=ae_outputs,name='VAE')\n",
    "\n",
    "    # Losses and optimization\n",
    "    my_adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute VAE loss\n",
    "    def my_vae_loss(y_true, y_pred):\n",
    "        reconstruction_loss = mse(K.flatten(y_true), K.flatten(y_pred))\n",
    "\n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        return vae_loss\n",
    "\n",
    "    model.compile(optimizer=my_adam, loss = my_vae_loss, metrics=[coeff_determination])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model, decoder, encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_def():\n",
    "    \n",
    "    def coeff_determination(y_pred, y_true): #Order of function inputs is important here        \n",
    "        SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "        SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "        return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "\n",
    "    def sampling(args):\n",
    "        \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "         Arguments\n",
    "            args (tensor): mean and log of variance of Q(z|X)\n",
    "         Returns\n",
    "            z (tensor): sampled latent vector\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        # by default, random_normal has mean=0 and std=1.0\n",
    "        epsilon = K.random_normal(shape=(batch, dim), mean=epsilon_mean, stddev=epsilon_std)\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    ## Encoder\n",
    "    encoder_inputs = Input(shape=(64,64,1),name='Field')\n",
    "    # Encode   \n",
    "    x = Conv2D(256,kernel_size=(3,3),activation='relu',padding='same')(encoder_inputs)\n",
    "    x = Conv2D(128,kernel_size=(2,2),activation='relu',padding='same')(encoder_inputs)\n",
    "    x = Conv2D(128,kernel_size=(1,1),activation='relu',padding='same')(encoder_inputs)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "    x = Conv2D(128,kernel_size=(3,3),activation='relu',padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "    # instantiate encoder model\n",
    "    encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    # build decoder model\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(512)(latent_inputs)\n",
    "    x = Reshape((16, 16, 2))(x)\n",
    "       \n",
    "\n",
    "\n",
    "    x = Conv2D(128,kernel_size=(3,3),activation='relu',padding='same')(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "#     x = Conv2D(256,kernel_size=(5,5),activation='relu',padding='same')(x)\n",
    "    x = Conv2D(256,kernel_size=(1,1),activation='relu',padding='same')(x)\n",
    "    x = Conv2D(256,kernel_size=(2,2),activation='relu',padding='same')(x)\n",
    "    x = Conv2D(256,kernel_size=(3,3),activation='relu',padding='same')(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "    decoded = Conv2D(1,kernel_size=(3,3),activation=None,padding='same')(x)\n",
    "    decoder = Model(inputs=latent_inputs,outputs=decoded)\n",
    "    decoder.summary()\n",
    "    # instantiate VAE model\n",
    "    ae_outputs = decoder(encoder(encoder_inputs))\n",
    "    model = Model(inputs=encoder_inputs,outputs=ae_outputs,name='VAE')\n",
    "\n",
    "#     # Losses and optimization\n",
    "#     my_adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)\n",
    "    \n",
    "#     # Compute VAE loss\n",
    "# #     def my_vae_loss(y_true, y_pred):\n",
    "#     def my_vae_loss(encoder_inputs, ae_outputs):\n",
    "\n",
    "#         reconstruction_loss = mse(K.flatten(ae_outputs), K.flatten(ae_outputs))\n",
    "\n",
    "#         kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "#         kl_loss = K.sum(kl_loss, axis=-1)\n",
    "#         kl_loss *= -0.5\n",
    "#         vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "#         return vae_loss\n",
    "#     model.compile(optimizer=my_adam, loss = my_vae_loss, metrics=[coeff_determination])\n",
    "\n",
    "\n",
    "    reconstruction_loss = mse(K.flatten(encoder_inputs), K.flatten(ae_outputs))\n",
    "\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    model.add_loss(vae_loss)\n",
    "    model.compile(optimizer='adam')\n",
    "    K.set_value(model.optimizer.lr, learning_rate)\n",
    "    K.set_value(model.optimizer.decay, decay_rate)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model, decoder, encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,decoder,encoder = model_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_filepath = 'best_weights_vae.h5'\n",
    "if mode == 'train':\n",
    "    checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min',save_weights_only=True)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "    callbacks_list = [checkpoint,earlystopping]\n",
    "#     train_history = model.fit(x=swe_train_data, y=swe_train_data, epochs=num_epochs, batch_size=batch_size, callbacks=callbacks_list, validation_split=0.1)\n",
    "\n",
    "    train_history = model.fit(swe_train_data, epochs=num_epochs, batch_size=batch_size, callbacks=callbacks_list, validation_split=0.1)\n",
    "    model.save_weights('vae_cnn')\n",
    "    print('Training complete')\n",
    "        # model.load_weights(weights_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_history = model.fit(x=swe_train_data, y=swe_train_data, epochs=num_epochs, batch_size=batch_size, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'train':\n",
    "     fig1 = plt.figure()\n",
    "     plt.plot(train_history.history['loss'],'r')\n",
    "     plt.plot(train_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = model.predict(swe_valid[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generator.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = 6\n",
    "\n",
    "f, a = plt.subplots(1, 3, figsize = (16,5))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, hspace=None)\n",
    "\n",
    "a[0].imshow(generator[indx,:,:,0])\n",
    "\n",
    "a[1].imshow(swe_valid[indx,:,:,0])\n",
    "\n",
    "a[2].imshow(generator[indx,:,:,0] - swe_valid[indx,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_train = model.predict(swe_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = 3\n",
    "plt.imshow(generator_train[indx,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('env_py37': conda)",
   "language": "python",
   "name": "python37564bitenvpy37conda17150a11545f4ac4a3ea6d7b215219bc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
